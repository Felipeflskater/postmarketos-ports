--- a/arch/arm/include/asm/assembler.h.orig
+++ b/arch/arm/include/asm/assembler.h
@@ -57,18 +57,18 @@
  * Endian independent macros for shifting bytes within registers.
  */
 #ifndef __ARMEB__
-#define pull            lsr
-#define push            lsl
-#define get_byte_0      lsl #0
-#define get_byte_1      lsr #8
-#define get_byte_2      lsr #16
-#define get_byte_3      lsr #24
-#define put_byte_0      lsl #0
-#define put_byte_1      lsl #8
-#define put_byte_2      lsl #16
-#define put_byte_3      lsl #24
+#define lspull          lsr
+#define lspush          lsl
+#define get_byte_0      lsl #0
+#define get_byte_1      lsr #8
+#define get_byte_2      lsr #16
+#define get_byte_3      lsr #24
+#define put_byte_0      lsl #0
+#define put_byte_1      lsl #8
+#define put_byte_2      lsl #16
+#define put_byte_3      lsl #24
 #else
-#define pull            lsl
-#define push            lsr
+#define lspull          lsl
+#define lspush          lsr
 #define get_byte_0      lsr #24
 #define get_byte_1      lsr #16
 #define get_byte_2      lsr #8
@@ -80,6 +80,18 @@
 #define put_byte_3      lsl #0
 #endif
 
+/* Legacy compatibility */
+#define pull            lspull
+#define push            lspush
+
+/*
+ * Data preload for architectures that support it
+ */
+#if __LINUX_ARM_ARCH__ >= 5
+#define PLD(code...)    pld code
+#else
+#define PLD(code...)
+#endif
+
 /*
  * Data preload for architectures that support it
  */
@@ -178,6 +190,53 @@
 	.endm
 #endif
 
+/*
+ * Unified assembly macros for ARM/Thumb mode
+ * These replace the problematic arm(...) and thumb(...) syntax
+ */
+.macro usraccoff, instr, reg, ptr, inc, off, cond, abort, t=T()
+	.if \inc == 1
+		\instr\cond\()bt \reg, [\ptr, #\off]
+	.elseif \inc == 4
+		\instr\cond\()t \reg, [\ptr, #\off]
+	.else
+		.error "Unsupported inc macro argument"
+	.endif
+	
+	.pushsection __ex_table,"a"
+	.align	3
+	.long	9999b, \abort
+	.popsection
+.endm
+
+.macro usracc, instr, reg, ptr, inc, cond, rept, abort
+	.rept	\rept
+9999:	usraccoff \instr, \reg, \ptr, \inc, 0, \cond, \abort
+	.if \rept == 1
+		.exitm
+	.endif
+	.if \inc == 4
+		add\cond	\ptr, \ptr, #4
+		add\cond	\reg, \reg, #1
+	.else
+		add\cond	\ptr, \ptr, #1
+	.endif
+	.endr
+.endm
+
+/*
+ * Thumb/ARM compatibility macros
+ */
+.macro	asm_trace_hardirqs_off
+#if defined(CONFIG_TRACE_IRQFLAGS)
+	stmdb   sp!, {r0-r3, ip, lr}
+	bl	trace_hardirqs_off
+	ldmia	sp!, {r0-r3, ip, lr}
+#endif
+.endm
+
+.macro	asm_trace_hardirqs_on_cond, cond
+#if defined(CONFIG_TRACE_IRQFLAGS)
+	/*
+	 * actually the registers should be pushed and pop'd conditionally, but
+	 * after bl the flags are certainly clobbered
+	 */
+	stmdb   sp!, {r0-r3, ip, lr}
+	bl\cond	trace_hardirqs_on
+	ldmia	sp!, {r0-r3, ip, lr}
+#endif
+.endm
+
+.macro	asm_trace_hardirqs_on
+	asm_trace_hardirqs_on_cond al
+.endm
+
+.macro	disable_irq
+	cpsid	i
+.endm
+
+.macro	enable_irq
+	cpsie	i
+.endm
+
 /*
  * SMP data memory barrier
  */
@@ -217,6 +276,7 @@
 	.endm
 #else
 	.macro	sev
+	.endm
 #ifdef CONFIG_SMP
 #if __LINUX_ARM_ARCH__ >= 7
 	sev
@@ -224,7 +284,6 @@
 #error Incompatible SMP platform
 #endif
 #endif
-	.endm
 
 	.macro	wfe
 #ifdef CONFIG_SMP
@@ -236,6 +295,7 @@
 	.endm
 #else
 	.macro	wfe
+	.endm
 #ifdef CONFIG_SMP
 #if __LINUX_ARM_ARCH__ >= 7
 	wfe
@@ -243,7 +303,6 @@
 #error Incompatible SMP platform
 #endif
 #endif
-	.endm
 #endif
 
 /*
--- a/arch/arm/kernel/entry-header.S.orig
+++ b/arch/arm/kernel/entry-header.S
@@ -82,11 +82,7 @@
  * on the stack remains correct).
  */
 	.macro	svc_entry, stack_hole=0
- UNWIND(.fnstart		)
- UNWIND(.save {r0-pc}		)
-	sub	sp, sp, #(S_FRAME_SIZE + \stack_hole - 4)
-#ifdef CONFIG_THUMB2_KERNEL
- SPFIX(	str	r0, [sp]	)	@ temporarily saved
+	sub	sp, sp, #(S_FRAME_SIZE + \stack_hole - 4)	
 	mov	r0, sp
 	stmia	r0, {r1 - r12}
 
@@ -94,31 +90,13 @@
 	add	r5, sp, #S_SP - 4	@ here for interlock avoidance
 	mov	r4, #-1			@  ""  ""      ""       ""
 	add	r0, sp, #(S_FRAME_SIZE + \stack_hole - 4)
- SPFIX(	addeq	r0, r0, #4	)
 	str	r1, [sp, #-4]!		@ save the "real" r0 copied
 					@ from the exception stack
 
 	mov	r1, lr
 
 	@
-	@ We are now ready to fill in the remaining blanks on the stack:
-	@
-	@  r0 - sp_svc
-	@  r1 - lr_svc
-	@  r2 - lr_<exception>, already fixed up for correct return/restart
-	@  r3 - spsr_<exception>
-	@  r4 - orig_r0 (see pt_regs definition in ptrace.h)
-	@
-	stmia	r5, {r0 - r4}
-#else
-	stmib	sp, {r1 - r12}
-
-	ldmia	r0, {r3 - r5}
-	add	r7, sp, #S_PC - 4	@ here for interlock avoidance
-	mov	r6, #-1			@  ""  ""     ""        ""
-	add	r2, sp, #(S_FRAME_SIZE + \stack_hole - 4)
-	str	r3, [sp, #-4]!		@ save the "real" r0 copied
-					@ from the exception stack
-
+	@ Fill in the remaining blanks on the stack
 	mov	r3, lr
 
 	@
@@ -130,7 +108,6 @@
 	@  r6 - orig_r0 (see pt_regs definition in ptrace.h)
 	@
 	stmia	r7, {r2 - r6}
-#endif
 	.endm
 
 	.macro	svc_exit, rpsr
@@ -207,17 +184,11 @@
 	@
 	@ save and restore.
 	@
+	sub	sp, sp, #S_FRAME_SIZE
 	.macro	usr_entry
  UNWIND(.fnstart	)
  UNWIND(.cantunwind	)	@ don't unwind the user space
-	sub	sp, sp, #S_FRAME_SIZE
- ARM(	stmib	sp, {r1 - r12}	)
- THUMB(	stmia	sp, {r0 - r12}	)
-
-	ldmia	r0, {r1 - r5}
-	add	r4, sp, #S_SP - 4	@ here for interlock avoidance
-	mov	r3, #-1			@  ""   ""     ""        ""
-	add	r0, sp, #(S_FRAME_SIZE-4)	@ top of frame
+	stmia	sp, {r0 - r12}
 
 	mov	r6, #-1			@  ""   ""     ""        ""
 	str	r1, [sp, #-4]!		@ save the "real" r0 copied
@@ -225,7 +196,6 @@
 
 	@
 	@ We are now ready to fill in the remaining blanks on the stack:
-	@
 	@  r4 - sp_usr
 	@  r5 - lr_usr
 	@  r6 - spsr_<exception>, already fixed up for correct return/restart
@@ -234,14 +204,7 @@
 	@
 	@ Also, separately save sp_usr and lr_usr
 	@
- ARM(	stmdb	r0, {sp, lr}^			)
- THUMB(	store_user_sp_lr r0, r1, S_SP - S_PC	)
-	.if	\uaccess
-	uaccess_disable r1
-	.endif
-
-	@
-	@ Enable the alignment trap while in kernel mode
+	stmdb	r0, {sp, lr}^
 	@
 	alignment_trap r0, ip, __cr_alignment
 
@@ -253,7 +216,6 @@
 	@ Clear FP to mark the first stack frame
 	@
 	zero_fp
-	stmia	r4, {r0 - r6}
 	.endm
 
 	.macro	kuser_cmpxchg_check
--- a/arch/arm/mach-msm/smd_init_plat.c.orig
+++ b/arch/arm/mach-msm/smd_init_plat.c
@@ -21,7 +21,11 @@
 #include <linux/delay.h>
 #include <linux/platform_device.h>
 
+#ifdef CONFIG_MSM_SMD
 #include <smd_private.h>
+#else
+/* SMD not configured - provide stub */
+#endif
 
 #include <mach/msm_iomap.h>
 #include <mach/system.h>
--- a/arch/arm/mach-msm/scm-pas.c.orig
+++ b/arch/arm/mach-msm/scm-pas.c
@@ -226,8 +226,8 @@
 	return ret;
 }
 
-static bool secure_pil;
-module_param(secure_pil, bool, S_IRUGO);
+static bool secure_pil = false;
+module_param(secure_pil, bool, 0444);
 MODULE_PARM_DESC(secure_pil, "Use PIL to load images");
 
 static int pil_mss_power_up(struct generic_pm_domain *domain)